同步和异步指的是应用程序和内核的交互方式，也就是内核的消息通知机制。
同步是指用户进程触发IO操作后，【等待（IO操作阻塞）】返回结果或者【轮询查看（IO操作非阻塞）】IO操作是否就绪；
由应用程序去等待IO操作是否完成。

异步是指用户进程触发IO操作后，操作立即返回，用户进程可以继续执行自己的其他任务。当IO操作在内核完成后
由内核通过通知机制（状态、回调、消息）去通知应用程序。

阻塞和非阻塞指的是应用程序在等待（IO操作处理结果的）消息通知时的状态。
阻塞是指IO操作的结果返回之前，应用程序的触发IO操作的线程会被挂起，一直处于等待消息通知，不能够执行其他任务。
非阻塞是指不能立刻得到IO操作结果之前，触发IO操作的线程不会被阻塞。


同步阻塞：
用户进程发起一个IO操作，不立即返回，必须等待内核IO操作完成后才知道处理结果。在此期间，操作线程是阻塞的，无法处理其他消息。

同步非阻塞：
用户进程发起一个IO操作，立即返回，但是必须定时轮询内核IO操作是否完成。

异步阻塞：
用户进程发起一个IO操作，立即返回，等内核IO操作完成以后会主动通知用户进程。但是在等待内核通知则是阻塞的，例如调用select
函数则会造成当前线程阻塞，不能处理其他事情，直到有消息通知为止。

异步非阻塞：
用户进程发起一个IO操作，立即返回，等内核IO操作完成以后会主动通知用户进程。内核通过回调、消息等机制通知用户进程，在此
期间，用户进程可以做其他的事情。


综上所述，同步和异步是相对于应用和内核的交互方式而言的，同步 需要主动去询问，
而异步的时候内核在IO事件发生的时候通知应用程序，
而阻塞和非阻塞仅仅是系统在调用系统调用的时候函数的实现方式而已。



同步阻塞IO中，线程实际上等待的时间可能包括两个部分：一个是等待数据就绪；另一个是等待数据的复制。
同步非阻塞IO的调用不会等待数据的就绪，如果数据不可读或不可写，会立即返回。因此需要轮询去检查数据的就绪状态。


////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////
Reactor模式：IO多路复用（select）【异步阻塞IO模型，阻塞在select上】
Proactor模式：异步IO

Reactor和Proactor模式的主要区别就是真正的读取和写入操作是有谁来完成的，Reactor中需要应用程序自己读取或者写入数据，
而Proactor模式中，应用程序不需要进行实际的读写过程，它只需要从缓存区读取或者写入即可，
操作系统会读取缓存区或者写入缓存区到真正的IO设备.






////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////
windows平台的IO模型：
1. 选择（Select）
2. 异步选择（WSAAsyncSelect）
3. 事件选择（WSAEventSelect）
4. 重叠IO（Overlapped IO）
5. 完成端口（Completion Port）



1. 选择（Select）模型
主要使用widnows的select函数。

在服务器端，默认情况下调用accept是阻塞的。
将socket句柄与fd_set关联
然后判断fd_set的状态，从而做出动作。


2. 异步选择（WSAAsyncSelect）模型
依赖于窗口过程，通过WSAAsyncSelect(sListen, hwnd, WM_SOCKET, FD_ACCEPT);
将SOCKET与消息WM_SOCKET（自定义）绑定，同时指定我们关心的模式（FD_ACCEPT）。
当接收到WM_SOCKET时，可以对参数WSAGETSELECTEVENT(lParam))进行判断：
 switch (WSAGETSELECTEVENT(lParam))
    {
    case FD_ACCEPT:
      // Accept a connection from client
      sClient = accept(wParam, (struct sockaddr *)&client, &iAddrSize);
      
      // Associate client socket with FD_READ and FD_CLOSE event
      WSAAsyncSelect(sClient, hwnd, WM_SOCKET, FD_READ | FD_CLOSE);
      break;

    case FD_READ:
      ret = recv(wParam, szMessage, MSGSIZE, 0);

      if (ret == 0 || ret == SOCKET_ERROR && WSAGetLastError() == WSAECONNRESET)
      {
        closesocket(wParam);
      }
      else
      {
        szMessage[ret] = '/0';
        send(wParam, szMessage, strlen(szMessage), 0);
      }
      break;
      
    case FD_CLOSE:
      closesocket(wParam);      
      break;
    }


3. 事件选择（WSAEventSelect）模型
将每个socket和WSAEVENT对象关联。并且在关联的时候指定需要关注的哪些网络事件。一旦在某个套接字上发生了我们关注的事件
（FD_READ和FD_CLOSE），与之相关联的WSAEVENT对象被Signaled。

在工作线程里，使用WaitForXXX的形式的WSAWaitForMultipleEvents来等待设置的事件。



4. 重叠IO（Overlapped IO）模型
有两种实现方式：使用事件通知；使用完成例程。
只是内核通知应用的方式不一样，一个使用事件，一个使用回调。
同时使用了一个异步的操作（WSARecv）

使用事件通知：
创建WSAOVERLAPPED::hEvent，并同时保存到全局数组中。
调用WSARecv触发一个异步操作。
在工作线程里，再使用WSAWaitForMultipleEvents。
有通知后，使用WSAGetOverlappedResult获取结果。再次调用WSARecv触发一个异步操作。

使用完成例程：
就是在WSARecv的最后一个参数中传入一个回调函数。同时需要注意的是，在工作现场调用WSARecv异步操作后，需要调用
SleepEx(1000, TRUE);来提示此线程是可通知的。并且内核可以使用我们提供的回调进行通知。有以下几种情况：
・ An I/O completion callback function is called
・ An asynchronous procedure call (APC) is queued to the thread.
・ The time-out interval elapses



5. 完成端口（Completion Port）模型




模型比较：
1. 线程数
选择、事件选择、用事件通知实现的重叠IO都有每个线程最大64个连接数的限制。如果连接数太大，需要对客户端socket进行分组管理。
相反，异步选择、用完成例程实现的重叠IO、完成端口不受此限制。
2. 实现复杂度
3. 性能



////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////
Linux平台的IO模型：
1. 阻塞IO（blocking IO）
2. 非阻塞IO（nonblocking IO）
3. IO复用，select和poll（IO multiplexing）
4. 信号驱动IO（signal driven IO；SIGIO）
5. 异步IO（asynchronous IO）

前面四种均为同步IO






////////////////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////////////////
libuv

/* Handle types. */
typedef struct uv_loop_s uv_loop_t;
typedef struct uv_handle_s uv_handle_t;
typedef struct uv_stream_s uv_stream_t;
typedef struct uv_tcp_s uv_tcp_t;
typedef struct uv_udp_s uv_udp_t;
typedef struct uv_pipe_s uv_pipe_t;
typedef struct uv_tty_s uv_tty_t;
typedef struct uv_poll_s uv_poll_t;
typedef struct uv_timer_s uv_timer_t;
typedef struct uv_prepare_s uv_prepare_t;
typedef struct uv_check_s uv_check_t;
typedef struct uv_idle_s uv_idle_t;
typedef struct uv_async_s uv_async_t;
typedef struct uv_process_s uv_process_t;
typedef struct uv_fs_event_s uv_fs_event_t;
typedef struct uv_fs_poll_s uv_fs_poll_t;
typedef struct uv_signal_s uv_signal_t;
/* Request types. */
typedef struct uv_req_s uv_req_t;
typedef struct uv_getaddrinfo_s uv_getaddrinfo_t;
typedef struct uv_getnameinfo_s uv_getnameinfo_t;
typedef struct uv_shutdown_s uv_shutdown_t;
typedef struct uv_write_s uv_write_t;
typedef struct uv_connect_s uv_connect_t;
typedef struct uv_udp_send_s uv_udp_send_t;
typedef struct uv_fs_s uv_fs_t;
typedef struct uv_work_s uv_work_t;
/* None of the above. */
typedef struct uv_cpu_info_s uv_cpu_info_t;
typedef struct uv_interface_address_s uv_interface_address_t;
typedef struct uv_dirent_s uv_dirent_t;
handle
request


https://atlas.mindmup.com/akagi201/learning_libuv/index.html