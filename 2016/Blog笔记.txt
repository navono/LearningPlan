2016.6.26
标题：7 Prerequisites to Become a Software Architect
link：http://ithare.com/7-prerequisites-to-become-a-software-architect/

先决条件-1：受团队成员和管理层的尊重（Being respected BOTH by team members AND management）
先决条件0：有精通的技术或者足够聪明（Having technical expertise / being smart enough）
先决条件1：你想要成员架构师（You should want to do it）
先决条件2：你应该有走出舒适区的意愿（You should be willing to go out of your comfort zone）
先决条件3：你应该有倾听其他人想法的意愿（和能力）（You should be willing (and be able) to listen to the others）
先决条件4：你应该准备好去承认有些人在某些领域比你更精通（You should be ready to admit that people know better than you within their field）
先决条件5：你应该有能力从一些不相干的细节中跳出来（You should be able to isolate yourself from irrelevant details）



//////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////

2016.7.1
标题：TCP/IP Explained. A Bit
link：http://ithare.com/tcp-ip-explained-a-bit/

在网络中传输的数据都是基于IP数据包（Internet Protocol）。而IP数据包在传输过程中是会丢失的。
UDP与TCP都是基于IP数据包的传输层协议。UDP的数据头相对来说比较简单，在传输过程中也不对数据包的投递进行保证，
也不进行检测。因此是“不可靠”的传输协议。也正是因为这个特性，UDP在延迟方面有很好的表现。同时也将UDP的使用场景局限在：
比起数据丢失来说，数据的延迟更能造成损害的情况。也就是数据丢失不可怕，但是延迟不行！比如VoIP和一些实时传输协议
Real-Time Protocol。

UDP可以进行多播，但就目前而言，这只局限于在内部网里。路由器拒绝将多播的数据包向之外的子网传输。
UDP就延迟来说比较出色，但在纯粹的流量吞吐方面，基于UDP协议还是很难与TCP抗衡。

TCP的可靠性（reliability）
TCP的校验和是16位，也就是说，如果一个数据包随机丢失，那么存在1/65536的概率，这个校验和会和另外一个相同，导致
这个丢失的包检验不到。
这在实际中会有两个影响：
1.永远不要依赖原生的TCP传输的可靠性。如果传输的文件是比较重要的，那么最好是额外的做一些校验工作。比如SHA-1或者其他
的校验机制。实际上SHA-1也不是绝对的可靠，存在1/2的160的丢失概率。但这几乎可以忽略。
如果是用了SSL的TCP，那么我们就可以假设这个传输时可靠的。
2.在一些状态不好的链路中传输大文件时，最好是在一定大小的数据块进行检验。这样不用在出现错误时，导致整个文件被
重新传输，而只要传输相对应的错误数据块就行。

TCP的交互性（interactivity）
TCP不是为交互的通讯而构建的。因此，在时间顺序上的延迟不是个错误，而是一个特性。问题是当你需要交互通讯的时候你要
做些什么？

TCP提供了一些提供交互性的办法，比如TCP_NODELAY。但是有一点需要注意，TCP_NODELAY在每个平台的表现都不一样，这个需要
在针对的平台进行测试。

开启TCP_NODELAY的一个特性就是，调用那个send会立即将数据进行传输。而在TCP_NODELAY关闭的情况下，会在缓冲区满的情况下
才进行传输，这也就是Nagle算法的目的。如果开启了TCP_NODELAY，如果一次传输1个字节，那么就会有额外的40个字节的开销
（数据头），这也就要求开发者自己组装发送的数据。

另外一个问题就是链路的状态，也就是挂起（hang）状态。TCP没有提供一个检测链路是否挂起的机制。就拿浏览器而言，很多
时候会一直在转圈，页面也一直没有显示出来。这很可能是有些数据正在重传，从而导致了一些延时。如上面所说，这是TCP的
一个特性，但是在用户角度来看，却无法接受这样的延迟。
同时，TCP的重发时间是双倍的RTT，这也是一个需要关注的点。TCP提供了一个心跳机制来检测链路的状况。但是效果不是很理想。
在Windows平台上，SO_KEEPALIVE的检测时间是2小时，同时无法编程地设置，只能修改注册表，然后全系统有效。Linux同样也没有
这样的机制。
但是写一个自定义的心跳机制比写一个可靠的UDP协议定制版来说，花的时间还是少的，同时出错的概率也小。

TCP单通道吞吐量（single-channel throughput）
因为存在BDP（Bandwidth-Delay Product。延迟带宽积），所以每条TCP链路的最大传输是5MBit/s。但是TCP Windows scaling技术
会提升这个限值。但不是每个平台都默认开启，Vista之前的操作系统就是默认不开启。
即使这些都没问题，客户端的数据发送速度是否足够也是个问题，同时可能还有一些加密、解密的操作。

在实际中，有一个叫着“下载加速”的技术，其实大部分这个技术都是使用多条链路来提升下载速度。同时也会导致另一个问题，
就是如果服务端的通道是有限的话，通常情况下所有通道的数据包可能会丢失或者延迟，因此这个链路可能会被适当的限流。
这也意味着，一个有着两个限流的TCP链路在数据传输上大致是一条TCP链路的两倍。以牺牲其他客户为代价。


TCP数据丢失恢复力（packet loss resilience）
根据经验来说，当数据丢失率超过一定的比例，就可以认为链路本质上变得不可用。这个比例基本是在5-10%，在这个
比例之间的数据丢失率的链路就应该考虑是不正常的了。通常“最后一公里”的正常值是在0.01-0.1%之间。


一般来说，在LAN中，send和recv确实可能存在1对1的情况。也就是发送端调用send一次后，另外接收端会调用一次recv。
但是在TCP协议中没有这样的规定，因为TCP是基于流的协议。因为流是没有边界的，多次send可能对应任意次
recv。

在WAN中这种情况就很常见。所以如果需要在消息间建立边界，那么应该在TCP之上引入此功能。
还有一个问题就是在网络传输过程中的数据的排列


TCP的PMTUD（Path MTU Discovery）
PMTUD是TCP的一个特性，就是在客户端和服务器之间的链路检测可以传输的最大数据包大小。这样可以增大数据的
吞吐量。而有时可能因为路由器或者防火墙的配置问题，导致可能此特性失效。TCP_NODELAY可以解决这个问题，
但是需要注意的是每次send的字节数最大为512字节（经验值）


最后就是测试和一些解决TCP链路问题的办法。wireshark是可很好的工具，同时能解析tcpdump的日志文件。
需要注意的是，在tcpdump上加上 -s 65535选项，否则在一些平台上tcpdump包可能会被截断。

测试中，对于高延迟的测试时很有必要的。





//////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////

2016.0703
标题：高性能数据传输
link：http://www.psc.edu/index.php/networking/641-tcp-tune#options

影响传输数据的传输速度有几个方面：
1. BDP（Bandwidth Delay Products）
2. 发送端和接收端的缓冲区大小（一般都是64KB）

BDP等于链路的传输最小速度*RTT

提高网络传输性能有以下选项：
1. 最大化TCP缓冲区（内存）大小
   有些系统会有一些全局性的设置来限值TCP连接可使用的系统内存的大小。有些系统会根据
   输入的数据、输出的数据以及控制变量来限制链路的内存
2. socket缓冲区大小
   大部分操作系统支持根据每个连接来设置发送和接收的缓冲区大小。BSB的setscokopt中的
   SO_SNDBUF和SO_RCVBUF。socket的缓冲区必须能够容纳TCP的BDP加上一些操作系统的开销。这些
   同样也决定了Receiver Window（rwnd），用于在TCP连接的两端实现流控。调整socket缓冲区大小
   有以下几种方式：
  a.TCP自动调整
  b.调整全局变量从而控制默认socket缓冲区大小。
  c.过大的缓冲区也可能会导致链路的交互性变得很差，同时也有过度使用内存的风险。所以建议轻微
  地大于64KB
  d.开发者可以使用setsockopt去设置socket缓冲区的大小
  
3. TCP大窗口扩展（Large Window Extensions。RFC1323）
   开启可选的TCP协议特性支持大BDP路径。window scale option（WSCALE）
4. TCP可选择的确认选项（Selective Acknowledgments Option。SACK，RFC2018）
    允许TCP接收方通知发送方那些数据已经丢失需要重传。
5. Path MTU


Windows下设置TCP参数的方法：
http://www.speedguide.net/articles/windows-2kxp-registry-tweaks-157
   

2016.0715
g3log源码分析

第一步：
先用静态方法createLogWorker创建LogWorker实例，LogWorker内部维护了一个LogWorkerImpl对象。
LogWorkerImpl内部保存了Active指针（异步消息队列，使用std::thread异步执行），同时还维护了一个
Sink的指针列表（使用父类SinkWrapper类型）。


第二步：
增加Sink，也就是实际上的Log处理方式。有两种方式，一是使用默认的addDefaultLogger；另一种是显示调用addSink
addDefaultLogger里默认使用FileSink，用户传入文件名相关的信息，回调方法是FileSink::fileWrite。

每当增加Sink的时候，都会将Sink实例传入到LogWorkerImpl内部的Sink指针列表保存，同时返回一个SinkHandle指针用于外部
使用（内部有一个Sink实例的弱引用）。

Sink内部会有一个后台的Active用于执行任务


第三步：
调用initializeLogging，传入LogWorker实例初始化全局变量。


第四步：
使用宏记录Log信息。可使用的宏包括：LOGF、LOGF_IF、LOG、LOG_IF、CHECK、CHECKF
这些宏内部都会使用LogCapture进行消息的格式化，在LogCapture析构方法里，调用internal空间的saveMessage对消息进行设置，
包括构造LogMessage对象，如果是Fatal消息，则调用fatalCall，否则调用pushMessageToLogger。
在pushMessageToLogger里，使用第三步传入的LogWorker实例的save方法进行保存。
fatalCall调用pushFatalMessageToLogger，使用LogWorker的fatal方法进行保存。


在LogWorkerImpl中，最终会遍历Sink队列，调用Sink的send方法保存消息。Sink的后台线程会调用之前保存的回调进行实际保存。
在FileSink中，析构时会调用std::flush，将数据通过std::ofstream更新到磁盘中
  

2016.0723
http://www.modernescpp.com/
http://www.jellythink.com/archives/878							C++设计模式
https://github.com/rigtorp/awesome-modern-cpp
http://cppcast.com/
  
  
  
  
  
  
  



